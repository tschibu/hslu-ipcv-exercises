{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "In this exercise, we will look at an introduction of classifying images using deep convolutional neural networks.\n",
    "\n",
    "Training convolutional networks on many images can be quite time consuming. We use a small standard dataset, that gives a good introduction to the topic and is available straight in Keras. It is also used in many classes and tutorials about image classification.\n",
    "\n",
    "The CIFAR-10 data set consists of 50000 training and 10000 test images of size 32x32 labelled into 10 classes. See also https://keras.io/datasets/#cifar10-small-image-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keras routines below might generate a ssl error inside jupyter notebooks\n",
    "#\n",
    "# (x_train, y_train_label), (x_test, y_test_label) = cifar10.load_data()\n",
    "# \n",
    "\n",
    "# this is the same code, but using http instead of https\n",
    "from tensorflow.python.keras.datasets.cifar import load_batch\n",
    "\n",
    "def load_cifar_data():\n",
    "    \"\"\"Loads CIFAR10 dataset.\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    dirname = 'cifar-10-batches-py'\n",
    "    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    path = keras.utils.get_file(dirname, origin=origin, untar=True)\n",
    "\n",
    "    num_train_samples = 50000\n",
    "\n",
    "    x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n",
    "    y_train = np.empty((num_train_samples,), dtype='uint8')\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        fpath = os.path.join(path, 'data_batch_' + str(i))\n",
    "        (x_train[(i - 1) * 10000: i * 10000, :, :, :],\n",
    "         y_train[(i - 1) * 10000: i * 10000]) = load_batch(fpath)\n",
    "\n",
    "    fpath = os.path.join(path, 'test_batch')\n",
    "    x_test, y_test = load_batch(fpath)\n",
    "\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        x_train = x_train.transpose(0, 2, 3, 1)\n",
    "        x_test = x_test.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_int, y_train_label), (x_test_int, y_test_label) = load_cifar_data()\n",
    "print(x_train_int.shape)\n",
    "print(y_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the training images. The images contain 10 different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "def show_images():\n",
    "    fig, axes = plt.subplots(3, 5)\n",
    "    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        example = np.random.randint(len(x_train_int))\n",
    "        ax.imshow(x_train_int[example])\n",
    "        \n",
    "        # Name of the true class.\n",
    "        cls_true_name = labels[y_train_label[example, 0]]\n",
    "        xlabel = \"class: {0}\".format(cls_true_name)\n",
    "        \n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the loss function (cross entropy) the label data set must be one-hot encoded (also called categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train_label)\n",
    "y_test = keras.utils.to_categorical(y_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Any input to a neural network should be normalized. For images this could be done per image (which would also normalize contrast) or just adjust the range globally. Normalization should either be so that the data is between 0 and 1 or (even better) between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (2.0 * x_train_int.astype(np.float32) / 255.0) - 1.0\n",
    "x_test = (2.0 * x_test_int.astype(np.float32) / 255.0) - 1.0\n",
    "print(np.min(x_train), np.max(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code calculates a simple convolutional network. \n",
    "\n",
    "For image classification, we want to have some convolutional layers, some padding and probably at the end some dense (fully connected) layers.\n",
    "\n",
    "The last layer needs softmax as activation function for the correct number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1():\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # add input shape\n",
    "    model.add(keras.layers.Layer(input_shape=(32, 32, 3)))\n",
    "    \n",
    "    # add convolutional and pooling layers first\n",
    "    model.add(keras.layers.Conv2D(32,3, padding='valid', activation='relu'))\n",
    "    # add a pooling layer and another  convolutional layer here\n",
    "    model.add(keras.layers.Conv2D(32,3, padding='valid', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(32,3, padding='valid', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    \n",
    "    # add dense layers (need to flatten the input first)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "  \n",
    "    # add the last layer for the number of classes and using softmax\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=2, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further with the model\n",
    "\n",
    "Even a very simple network gets some decent results. From here we can go further using deeper networks and additional features.\n",
    "\n",
    "Try the following:\n",
    "- Make the network deeper by adding more layers\n",
    "- Increase the capacity by adding more nodes per layer\n",
    "- add regularization\n",
    "- add dropout\n",
    "\n",
    "Convolutional networks benefit from using GPU resources. Google provides a jupyter notebook environment with GPU support: https://colab.research.google.com\n",
    "\n",
    "This jupyter notebook should run as it is in that environment. To actually get a GPU you have to change the runtime type in the menu 'Runtime'\n",
    "\n",
    "When building new models, I usually like to keep the old ones for comparison, so I would recommend defining a new function for the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2():\n",
    "     model = keras.Sequential()\n",
    "     # todo\n",
    "     return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Keras supports different optimization models to use in model.compile, see https://keras.io/optimizers/.\n",
    "\n",
    "- Try different parameters for the SGD like initial learning rate and decay.\n",
    "- Try some of the oder popular optimization functions, like RMSprop, AdaGrad and Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
